{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.datasets import cifar10 \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import regularizers\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CIFAR10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "imageDataGen = ImageDataGenerator(rotation_range = 90, width_shift_range = 0.1, height_shift_range = 0.1, horizontal_flip = True)\n",
    "imageDataGen.fit(x_train)\n",
    "\n",
    "classes = 10\n",
    "y_train = to_categorical(y_train, classes)\n",
    "y_test = to_categorical(y_test, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "weight_decay = 1e-4\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    " \n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    " \n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dense(classes, activation='softmax'))\n",
    " \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "781/781 [==============================] - 127s 161ms/step - loss: 0.8313 - accuracy: 0.7693 - val_loss: 1.0265 - val_accuracy: 0.7294\n",
      "Epoch 2/100\n",
      "781/781 [==============================] - 125s 161ms/step - loss: 0.8290 - accuracy: 0.7718 - val_loss: 0.9308 - val_accuracy: 0.7455\n",
      "Epoch 3/100\n",
      "781/781 [==============================] - 125s 160ms/step - loss: 0.8248 - accuracy: 0.7753 - val_loss: 0.8850 - val_accuracy: 0.7629\n",
      "Epoch 4/100\n",
      "781/781 [==============================] - 125s 160ms/step - loss: 0.8392 - accuracy: 0.7702 - val_loss: 0.9296 - val_accuracy: 0.7545\n",
      "Epoch 5/100\n",
      "781/781 [==============================] - 125s 160ms/step - loss: 0.8218 - accuracy: 0.7729 - val_loss: 0.9700 - val_accuracy: 0.7469\n",
      "Epoch 6/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.8334 - accuracy: 0.7738 - val_loss: 1.0374 - val_accuracy: 0.7348\n",
      "Epoch 7/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.8414 - accuracy: 0.7678 - val_loss: 0.9172 - val_accuracy: 0.7516\n",
      "Epoch 8/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8316 - accuracy: 0.7724 - val_loss: 0.8261 - val_accuracy: 0.7812\n",
      "Epoch 9/100\n",
      "781/781 [==============================] - 123s 157ms/step - loss: 0.8325 - accuracy: 0.7718 - val_loss: 0.8307 - val_accuracy: 0.7752\n",
      "Epoch 10/100\n",
      "781/781 [==============================] - 123s 157ms/step - loss: 0.8298 - accuracy: 0.7719 - val_loss: 0.8238 - val_accuracy: 0.7840\n",
      "Epoch 11/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8265 - accuracy: 0.7725 - val_loss: 0.8152 - val_accuracy: 0.7922\n",
      "Epoch 12/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8272 - accuracy: 0.7760 - val_loss: 0.9751 - val_accuracy: 0.7444\n",
      "Epoch 13/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8324 - accuracy: 0.7690 - val_loss: 1.0686 - val_accuracy: 0.7085\n",
      "Epoch 14/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8293 - accuracy: 0.7729 - val_loss: 0.8279 - val_accuracy: 0.7859\n",
      "Epoch 15/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8287 - accuracy: 0.7744 - val_loss: 0.8703 - val_accuracy: 0.7755\n",
      "Epoch 16/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.8230 - accuracy: 0.7747 - val_loss: 0.9536 - val_accuracy: 0.7415\n",
      "Epoch 17/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8263 - accuracy: 0.7726 - val_loss: 0.9451 - val_accuracy: 0.7330\n",
      "Epoch 18/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8294 - accuracy: 0.7751 - val_loss: 0.8973 - val_accuracy: 0.7566\n",
      "Epoch 19/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8282 - accuracy: 0.7749 - val_loss: 0.9462 - val_accuracy: 0.7539\n",
      "Epoch 20/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.8301 - accuracy: 0.7722 - val_loss: 0.8118 - val_accuracy: 0.7840\n",
      "Epoch 21/100\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.8206 - accuracy: 0.7754 - val_loss: 0.9461 - val_accuracy: 0.7440\n",
      "Epoch 22/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.8272 - accuracy: 0.7743 - val_loss: 1.0551 - val_accuracy: 0.7296\n",
      "Epoch 23/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.8257 - accuracy: 0.7720 - val_loss: 0.8940 - val_accuracy: 0.7725\n",
      "Epoch 24/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8324 - accuracy: 0.7715 - val_loss: 0.7975 - val_accuracy: 0.7910\n",
      "Epoch 25/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.8248 - accuracy: 0.7751 - val_loss: 0.9104 - val_accuracy: 0.7576\n",
      "Epoch 26/100\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.8334 - accuracy: 0.7738 - val_loss: 0.8633 - val_accuracy: 0.7763\n",
      "Epoch 27/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.8272 - accuracy: 0.7739 - val_loss: 0.8275 - val_accuracy: 0.7796\n",
      "Epoch 28/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.8400 - accuracy: 0.7728 - val_loss: 0.8922 - val_accuracy: 0.7674\n",
      "Epoch 29/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.8305 - accuracy: 0.7724 - val_loss: 0.9987 - val_accuracy: 0.7384\n",
      "Epoch 30/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.8295 - accuracy: 0.7721 - val_loss: 0.8910 - val_accuracy: 0.7620\n",
      "Epoch 31/100\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.8290 - accuracy: 0.7736 - val_loss: 0.8805 - val_accuracy: 0.7748\n",
      "Epoch 32/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.8245 - accuracy: 0.7780 - val_loss: 0.9427 - val_accuracy: 0.7526\n",
      "Epoch 33/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.8202 - accuracy: 0.7763 - val_loss: 0.8618 - val_accuracy: 0.7775\n",
      "Epoch 34/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.8296 - accuracy: 0.7732 - val_loss: 0.9674 - val_accuracy: 0.7473\n",
      "Epoch 35/100\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.8239 - accuracy: 0.7747 - val_loss: 0.8100 - val_accuracy: 0.7902\n",
      "Epoch 36/100\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.8220 - accuracy: 0.7736 - val_loss: 0.8516 - val_accuracy: 0.7707\n",
      "Epoch 37/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8212 - accuracy: 0.7752 - val_loss: 0.9037 - val_accuracy: 0.7603\n",
      "Epoch 38/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8186 - accuracy: 0.7776 - val_loss: 0.9298 - val_accuracy: 0.7604\n",
      "Epoch 39/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8167 - accuracy: 0.7788 - val_loss: 1.0832 - val_accuracy: 0.7196\n",
      "Epoch 40/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8162 - accuracy: 0.7772 - val_loss: 0.8496 - val_accuracy: 0.7735\n",
      "Epoch 41/100\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.8306 - accuracy: 0.7713 - val_loss: 0.8777 - val_accuracy: 0.7691\n",
      "Epoch 42/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8311 - accuracy: 0.7724 - val_loss: 0.8006 - val_accuracy: 0.7907\n",
      "Epoch 43/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8310 - accuracy: 0.7712 - val_loss: 0.8332 - val_accuracy: 0.7847\n",
      "Epoch 44/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.8255 - accuracy: 0.7736 - val_loss: 0.8672 - val_accuracy: 0.7759\n",
      "Epoch 45/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.8209 - accuracy: 0.7749 - val_loss: 0.7970 - val_accuracy: 0.7967\n",
      "Epoch 46/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.8148 - accuracy: 0.7756 - val_loss: 0.8625 - val_accuracy: 0.7669\n",
      "Epoch 47/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.8249 - accuracy: 0.7731 - val_loss: 0.8170 - val_accuracy: 0.7770\n",
      "Epoch 48/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8203 - accuracy: 0.7745 - val_loss: 0.8327 - val_accuracy: 0.7890\n",
      "Epoch 49/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8206 - accuracy: 0.7757 - val_loss: 0.8883 - val_accuracy: 0.7639\n",
      "Epoch 50/100\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.8268 - accuracy: 0.7727 - val_loss: 0.8692 - val_accuracy: 0.7697\n",
      "Epoch 51/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8226 - accuracy: 0.7742 - val_loss: 0.8333 - val_accuracy: 0.7834\n",
      "Epoch 52/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8233 - accuracy: 0.7754 - val_loss: 0.8537 - val_accuracy: 0.7741\n",
      "Epoch 53/100\n",
      "781/781 [==============================] - 123s 157ms/step - loss: 0.8194 - accuracy: 0.7718 - val_loss: 0.8136 - val_accuracy: 0.7921\n",
      "Epoch 54/100\n",
      "781/781 [==============================] - 122s 157ms/step - loss: 0.8265 - accuracy: 0.7741 - val_loss: 0.8769 - val_accuracy: 0.7658\n",
      "Epoch 55/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8192 - accuracy: 0.7759 - val_loss: 0.8266 - val_accuracy: 0.7824\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 122s 157ms/step - loss: 0.8206 - accuracy: 0.7742 - val_loss: 0.9092 - val_accuracy: 0.7638\n",
      "Epoch 57/100\n",
      "781/781 [==============================] - 122s 157ms/step - loss: 0.8279 - accuracy: 0.7708 - val_loss: 0.9669 - val_accuracy: 0.7398\n",
      "Epoch 58/100\n",
      "781/781 [==============================] - 122s 156ms/step - loss: 0.8233 - accuracy: 0.7746 - val_loss: 0.7811 - val_accuracy: 0.7970\n",
      "Epoch 59/100\n",
      "781/781 [==============================] - 123s 157ms/step - loss: 0.8217 - accuracy: 0.7768 - val_loss: 0.8051 - val_accuracy: 0.7903\n",
      "Epoch 60/100\n",
      "781/781 [==============================] - 123s 157ms/step - loss: 0.8217 - accuracy: 0.7768 - val_loss: 0.9411 - val_accuracy: 0.7513\n",
      "Epoch 61/100\n",
      "781/781 [==============================] - 122s 157ms/step - loss: 0.8220 - accuracy: 0.7775 - val_loss: 0.8454 - val_accuracy: 0.7742\n",
      "Epoch 62/100\n",
      "781/781 [==============================] - 123s 157ms/step - loss: 0.8113 - accuracy: 0.7782 - val_loss: 0.8912 - val_accuracy: 0.7716\n",
      "Epoch 63/100\n",
      "781/781 [==============================] - 123s 157ms/step - loss: 0.8277 - accuracy: 0.7733 - val_loss: 0.8308 - val_accuracy: 0.7850\n",
      "Epoch 64/100\n",
      "781/781 [==============================] - 123s 157ms/step - loss: 0.8294 - accuracy: 0.7703 - val_loss: 0.8104 - val_accuracy: 0.7879\n",
      "Epoch 65/100\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.8255 - accuracy: 0.7716 - val_loss: 0.9218 - val_accuracy: 0.7595\n",
      "Epoch 66/100\n",
      "781/781 [==============================] - 125s 160ms/step - loss: 0.8205 - accuracy: 0.7748 - val_loss: 0.8446 - val_accuracy: 0.7804\n",
      "Epoch 67/100\n",
      "781/781 [==============================] - 125s 160ms/step - loss: 0.8208 - accuracy: 0.7736 - val_loss: 0.8076 - val_accuracy: 0.7925\n",
      "Epoch 68/100\n",
      "781/781 [==============================] - 125s 160ms/step - loss: 0.8130 - accuracy: 0.7762 - val_loss: 0.9183 - val_accuracy: 0.7539\n",
      "Epoch 69/100\n",
      "781/781 [==============================] - 125s 160ms/step - loss: 0.8155 - accuracy: 0.7770 - val_loss: 0.9217 - val_accuracy: 0.7551\n",
      "Epoch 70/100\n",
      "781/781 [==============================] - 125s 160ms/step - loss: 0.8174 - accuracy: 0.7774 - val_loss: 0.9051 - val_accuracy: 0.7705\n",
      "Epoch 71/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8170 - accuracy: 0.7767 - val_loss: 0.8635 - val_accuracy: 0.7767\n",
      "Epoch 72/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8224 - accuracy: 0.7768 - val_loss: 0.8927 - val_accuracy: 0.7679\n",
      "Epoch 73/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8120 - accuracy: 0.7746 - val_loss: 0.8213 - val_accuracy: 0.7844\n",
      "Epoch 74/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8242 - accuracy: 0.7731 - val_loss: 0.8727 - val_accuracy: 0.7773\n",
      "Epoch 75/100\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.8159 - accuracy: 0.7761 - val_loss: 0.8341 - val_accuracy: 0.7802\n",
      "Epoch 76/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.8203 - accuracy: 0.7734 - val_loss: 0.8456 - val_accuracy: 0.7819\n",
      "Epoch 77/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.7693 - accuracy: 0.7933 - val_loss: 0.7501 - val_accuracy: 0.8101\n",
      "Epoch 78/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.7447 - accuracy: 0.7983 - val_loss: 0.7695 - val_accuracy: 0.8058\n",
      "Epoch 79/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.7498 - accuracy: 0.7976 - val_loss: 0.8194 - val_accuracy: 0.7911\n",
      "Epoch 80/100\n",
      "781/781 [==============================] - 125s 159ms/step - loss: 0.7420 - accuracy: 0.7993 - val_loss: 0.7567 - val_accuracy: 0.8003\n",
      "Epoch 81/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.7307 - accuracy: 0.8029 - val_loss: 0.7016 - val_accuracy: 0.8197\n",
      "Epoch 82/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.7329 - accuracy: 0.7986 - val_loss: 0.6949 - val_accuracy: 0.8191\n",
      "Epoch 83/100\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.7253 - accuracy: 0.8031 - val_loss: 0.7549 - val_accuracy: 0.8017\n",
      "Epoch 84/100\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.7158 - accuracy: 0.8058 - val_loss: 0.7363 - val_accuracy: 0.8131\n",
      "Epoch 85/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.7223 - accuracy: 0.8000 - val_loss: 0.7109 - val_accuracy: 0.8133\n",
      "Epoch 86/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.7063 - accuracy: 0.8049 - val_loss: 0.7208 - val_accuracy: 0.8122\n",
      "Epoch 87/100\n",
      "781/781 [==============================] - 124s 158ms/step - loss: 0.7141 - accuracy: 0.8021 - val_loss: 0.7351 - val_accuracy: 0.8077\n",
      "Epoch 88/100\n",
      "781/781 [==============================] - 123s 158ms/step - loss: 0.7062 - accuracy: 0.8053 - val_loss: 0.8310 - val_accuracy: 0.7792\n",
      "Epoch 89/100\n",
      "781/781 [==============================] - 126s 161ms/step - loss: 0.7123 - accuracy: 0.8001 - val_loss: 0.7240 - val_accuracy: 0.8085\n",
      "Epoch 90/100\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.7056 - accuracy: 0.8037 - val_loss: 0.7659 - val_accuracy: 0.8015\n",
      "Epoch 91/100\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.7109 - accuracy: 0.7991 - val_loss: 0.8516 - val_accuracy: 0.7796\n",
      "Epoch 92/100\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.7036 - accuracy: 0.8045 - val_loss: 0.7132 - val_accuracy: 0.8149\n",
      "Epoch 93/100\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.7049 - accuracy: 0.8048 - val_loss: 0.7375 - val_accuracy: 0.8080\n",
      "Epoch 94/100\n",
      "781/781 [==============================] - 125s 160ms/step - loss: 0.6983 - accuracy: 0.8037 - val_loss: 0.6806 - val_accuracy: 0.8158\n",
      "Epoch 95/100\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.7006 - accuracy: 0.8020 - val_loss: 0.8159 - val_accuracy: 0.7810\n",
      "Epoch 96/100\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.7000 - accuracy: 0.8033 - val_loss: 0.7344 - val_accuracy: 0.8031\n",
      "Epoch 97/100\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.6847 - accuracy: 0.8114 - val_loss: 0.7044 - val_accuracy: 0.8128\n",
      "Epoch 98/100\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.6970 - accuracy: 0.8040 - val_loss: 0.7825 - val_accuracy: 0.7964\n",
      "Epoch 99/100\n",
      "781/781 [==============================] - 125s 160ms/step - loss: 0.7076 - accuracy: 0.8022 - val_loss: 0.6812 - val_accuracy: 0.8196\n",
      "Epoch 100/100\n",
      "781/781 [==============================] - 124s 159ms/step - loss: 0.6956 - accuracy: 0.8043 - val_loss: 0.7458 - val_accuracy: 0.8001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19cb651b040>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    lrate = 0.001\n",
    "    if epoch > 75:\n",
    "        lrate = 0.0005 \n",
    "    return lrate\n",
    "\n",
    "batch_size = 64  \n",
    "opt_RMS = optimizers.RMSprop(lr=0.001,decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer = opt_RMS, metrics = ['accuracy'])\n",
    "model.fit_generator(imageDataGen.flow(x_train, y_train, batch_size = batch_size),\\\n",
    "                    steps_per_epoch = x_train.shape[0] // batch_size, epochs = 100,\\\n",
    "                    verbose = 1, validation_data = (x_test, y_test), callbacks=[LearningRateScheduler(lr_schedule)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved as models/CIFAR10_CNN.h5\n"
     ]
    }
   ],
   "source": [
    "CIFAR10_CNN = 'models/CIFAR10_CNN.h5'\n",
    "model.save(CIFAR10_CNN)\n",
    "print('model saved as', CIFAR10_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 4s 50ms/step - loss: 0.7458 - accuracy: 0.8001\n",
      "Accuracy: 80.01 loss: 0.75\n"
     ]
    }
   ],
   "source": [
    "m_eval = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
    "print('Accuracy: %.2f loss: %.2f' % (m_eval[1]*100, m_eval[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
